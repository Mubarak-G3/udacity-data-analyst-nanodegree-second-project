{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project on WeRateDogs Twitter Data.\n",
    "\n",
    "### Wrangle Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset wrangled in the project is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. \n",
    "WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. \n",
    "\n",
    "The WeRateDogs Twitter project goals included: \n",
    "Wrangling the twitter data through the following processes:\n",
    "- Gathering data\n",
    "- Assessing data\n",
    "- Cleaning data\n",
    "- Storing, analyzing, and visualizing your wrangled data\n",
    "- Reporting on the data wrangling efforts and data analyses and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Gathering the Data\n",
    "Three sources of data were gathered to complete this project:\n",
    "\n",
    "1. Twitter enhanced archive file - csv downloaded from udacity course site, which includes various information about tweets of WeRateDogs account\n",
    "2. Tweet image predictions file - tsv requested from udacity site, which includes predictions of objects on images included in WeRateDogs tweets\n",
    "3. Tweet details file - json file downloaded from twitter using API, which includes information missing from the enhanced archive file, namely retweets and likes counts\n",
    "\n",
    "Data was gathered using different methods:\n",
    "- csv was read from a file and stored in archive\n",
    "- tsv was downloaded using the requests library, written to a file and stored in images\n",
    "- json was gathered using twitter API, stored as txt file, loaded using json library and finally saved as tweet_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Assessing the Data\n",
    "After gathering the above three dataframes we moved straight to assessing them for quality and tidiness issues, in which we uncovered nine quality issues and two tidiness issues. They are as follows:\n",
    "\n",
    "\n",
    "\n",
    "##### The two types of Data Assessment performed\n",
    "\n",
    "- Visual assessment: Each piece of gathered data is displayed in the Jupyter Notebook. Once displayed, data are additionally assessed in an external application (Excel, text file reader)\n",
    "- Programmatic assessment: pandasâ€™ functions and/or methods are used to assess the data.\n",
    "\n",
    "###### Quality Issues \n",
    "- tweet_id is an integer\n",
    "\n",
    "- timestamp is of 'object' datatype.\n",
    "\n",
    "- name has values that are string 'None' instead of NaN and some values have unusual names of less than 3 characters such as 'a'.\n",
    "\n",
    "- NaNs represented as 'None' (str) for name, doggo, floofer, pupper, and puppo columns.\n",
    "\n",
    "- Archive data contains retweets.\n",
    "\n",
    "- Some of the ratings are wrongly mentioned e.g. in one case, the rating should've been 13/10, not 960/00, while some tweets are not about dogs, so doesn't contain rating (tweets not containing dog images can be discarded), and some of the ratings contain decimal in the numerator.\n",
    "\n",
    "- There are some missing rows in images dataset (2075 rows instead of 2356): either the rows are missing or some tweets didn't have dog images.\n",
    "\n",
    "- There are some duplicate jpg_urls.\n",
    "\n",
    "- p1, p2, and p3 contains underscores instead of spaces in the string.\n",
    "\n",
    "\n",
    "\n",
    "###### Tidiness Issues\n",
    "- The different dataframes should be merged into a single one.\n",
    "\n",
    "- There are 4 different columns (doggo, floofer, pupper, and puppo) for dog stages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cleaning the Data\n",
    "First Step: I have copied all the three DataFrames using .copy() method,\n",
    "\n",
    "- archive_clean = archive.copy()\n",
    "- images_clean = images.copy()\n",
    "- tweets_clean = tweets_table.copy()\n",
    "\n",
    "Further Steps:\n",
    "- This is the action stage. here I put all of the above observations into action and used the the given cleaning mehtod to adreess all the above issues.\n",
    "- Taking each issue separately, I used the Define, Code and Test method to firstly tackle the tidiness issues as they are wider and cover the whole dataset, then follow up with each of the quality issues.\n",
    "- Each issue was handled separately using the define-code-test workflow.\n",
    "- The cleaning efforts finished with storing the final merged data set as twitter_archive_master.csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
